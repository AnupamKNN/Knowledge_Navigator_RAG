{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fae9a0",
   "metadata": {},
   "source": [
    "## Multi-Agent GenAI Knowledge Navigator for Workforce Upskilling\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Organizations face a critical challenge: ensuring their workforce keeps pace with rapidly evolving domains such as Data Analytics, Data Science, Generative AI, Computer Vision, and Project Management. Traditional upskilling methods rely heavily on static training manuals, lengthy books, or outdated PDFs, which:\n",
    "\n",
    "* Delay the learning cycle and overwhelm employees with information overload.\n",
    "* Fail to connect foundational knowledge with the latest industry practices.\n",
    "* Limit scalability in delivering targeted, role-specific learning paths.\n",
    "\n",
    "This results in:\n",
    "\n",
    "* Slower workforce readiness and skill adoption.\n",
    "* Increased training costs with lower ROI.\n",
    "* Reduced competitiveness in industries driven by fast-paced innovation.\n",
    "\n",
    "### Use Case\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) platform powered by multi-agent AI to:\n",
    "\n",
    "* Ingest technical books, knowledge PDFs, and compliance guides across upskilling domains.\n",
    "* Summarize dense material into **role-specific microlearning modules**.\n",
    "* Cross-reference knowledge with **live internet sources** to ensure freshness and regulatory alignment.\n",
    "* Auto-generate **interview preparation content, study aids, and practice questions** tailored to workforce roles.\n",
    "\n",
    "### Example Use Case Scenarios\n",
    "\n",
    "* Transforming a **300-page Data Science guide** into structured modules for analysts, engineers, and managers.\n",
    "* Building **GenAI and Computer Vision upskilling roadmaps** that merge foundational book knowledge with current real-world advancements.\n",
    "* Preparing employees for **project management certifications** by combining official manuals with live case studies and scenario-based assessments.\n",
    "\n",
    "### Who Benefits\n",
    "\n",
    "* **Employees & Job Seekers:** Gain faster, validated, and targeted knowledge aligned with role expectations.\n",
    "* **Upskilling Programs:** Deliver scalable, domain-specific learning paths for workforce readiness.\n",
    "* **Organizations:** Improve training ROI, accelerate employee onboarding, and ensure skill competitiveness.\n",
    "* **Lifelong Learners:** Access both curated book knowledge and real-time industry updates.\n",
    "\n",
    "### In Summary\n",
    "\n",
    "This solution delivers an **intelligent knowledge navigator** that extracts, synthesizes, and validates learning material‚Äîempowering organizations to build a future-ready workforce through smarter, faster, and continuously updated upskilling programs.\n",
    "\n",
    "### üéØ Project Objective\n",
    "\n",
    "* Develop a production-ready, cloud-native, multi-agent GenAI platform that ingests books and knowledge PDFs, indexes them in a scalable vector database (ChromaDB), and enables hybrid retrieval with real-time internet search.\n",
    "* Enable end-to-end corporate learning workflows: book ingestion, chapter-level summarization, role-based learning paths, interview prep, and knowledge validation.\n",
    "* Incorporate analytics, reporting, and feedback loops to measure learning impact and optimize upskilling strategies.\n",
    "\n",
    "### Key Objectives\n",
    "\n",
    "* Provide rapid summaries and knowledge drill-downs mapped to workforce roles.\n",
    "* Generate validated, role-specific interview and certification prep content.\n",
    "* Track knowledge gaps across teams and recommend targeted resources.\n",
    "* Customize upskilling across domains like AI, cloud, data, and project management.\n",
    "\n",
    "### ‚ùì Core Questions Addressed\n",
    "\n",
    "* How can large books and technical PDFs be transformed into **role-specific microlearning modules**?\n",
    "* How can organizations ensure employees‚Äô knowledge stays **aligned with current industry trends and regulations**?\n",
    "* What **domain-specific interview and certification content** should employees prepare for today?\n",
    "* How can corporate upskilling be made more **scalable, measurable, and adaptive**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585635a8",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "Before we begin, it‚Äôs essential to load all necessary libraries for PDF processing, text splitting, and general data handling.\n",
    "\n",
    "These libraries will help us read PDFs, manage large documents, and organize data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29be079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36083e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309b707",
   "metadata": {},
   "source": [
    "### Define Data Source and File Paths\n",
    "\n",
    "Here, we specify the folder containing our PDF collection.\n",
    "\n",
    "Ensuring we point to the right directory is critical for the automation and scalability of ingestion.\n",
    "\n",
    "By organizing files in a dedicated folder, the system becomes maintainable and easily extensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398c0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 PDF files for ingestion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Generative AI_ A Beginner's Guide.pdf\",\n",
       " 'SQL Book.pdf',\n",
       " 'Maths$Stats_NOTES.docx.pdf',\n",
       " 'JIRASOFTWARESERVER071-290216.pdf',\n",
       " 'DAX Functions Cheat Sheet.pdf',\n",
       " 'Artificial Intelligence, Machine Learning, and Deep Learning.pdf',\n",
       " 'Project_Management_15694.pdf',\n",
       " 'Book_Power BI from Rookie to Rock Star_Book01_Power_BI_Essentials_Reza Rad_RADACAD.pdf',\n",
       " 'Probability_&_Stats_Q&A.pdf',\n",
       " 'Stats_Q&A.pdf',\n",
       " 'Computer Vision.pdf',\n",
       " 'excel_Basic_Microsoft_Excel_2010_YASHADA_Ver 1.pdf',\n",
       " 'Introduction_to_Python_Programming.pdf',\n",
       " 'SQL_Queries.pdf',\n",
       " 'How_to_explain_quantification.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the folder path containing the PDFs\n",
    "data_folder = \"data\"\n",
    "\n",
    "# List all PDF files available in this folder\n",
    "pdf_files = [f for f in os.listdir(data_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files for ingestion.\")\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c137e3e",
   "metadata": {},
   "source": [
    "### Load PDFs and Extract Raw Text\n",
    "\n",
    "In this step, we load each PDF document, extract its textual content, and store it for processing.\n",
    "\n",
    "Extracting text accurately from PDFs is a crucial challenge because documents may vary in structure and formatting.\n",
    "\n",
    "Our approach leverages the LangChain PyPDFLoader which robustly handles text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ba6981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: Generative AI_ A Beginner's Guide.pdf\n",
      "Loading file: SQL Book.pdf\n",
      "Loading file: Maths$Stats_NOTES.docx.pdf\n",
      "Loading file: JIRASOFTWARESERVER071-290216.pdf\n",
      "Loading file: DAX Functions Cheat Sheet.pdf\n",
      "Loading file: Artificial Intelligence, Machine Learning, and Deep Learning.pdf\n",
      "Loading file: Project_Management_15694.pdf\n",
      "Loading file: Book_Power BI from Rookie to Rock Star_Book01_Power_BI_Essentials_Reza Rad_RADACAD.pdf\n",
      "Loading file: Probability_&_Stats_Q&A.pdf\n",
      "Loading file: Stats_Q&A.pdf\n",
      "Loading file: Computer Vision.pdf\n",
      "Loading file: excel_Basic_Microsoft_Excel_2010_YASHADA_Ver 1.pdf\n",
      "Loading file: Introduction_to_Python_Programming.pdf\n",
      "Loading file: SQL_Queries.pdf\n",
      "Loading file: How_to_explain_quantification.pdf\n",
      "Loaded a total of 15 raw documents from PDFs.\n"
     ]
    }
   ],
   "source": [
    "# Load PDFs and extract text documents\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(data_folder, pdf_file)\n",
    "    print(f\"Loading file: {pdf_file}\")\n",
    "    loader = PyPDFLoader(file_path= file_path)\n",
    "    documents = loader.load()\n",
    "    all_docs.append(documents)\n",
    "\n",
    "print(f\"Loaded a total of {len(all_docs)} raw documents from PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb8818",
   "metadata": {},
   "source": [
    "### Split Documents into Chunks\n",
    "\n",
    "Long documents are split into smaller, digestible chunks to allow efficient embedding generation and search.\n",
    "\n",
    "The chunk size and overlap are tunable parameters that balance context retention with retrieval performance.\n",
    "\n",
    "Effective chunking prevents context loss during semantic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2263e",
   "metadata": {},
   "source": [
    "The choice of chunk_size=1000 and chunk_overlap=100 in RecursiveCharacterTextSplitter balances several important factors for handling large PDFs (e.g., books over 400 pages):\n",
    "\n",
    "- Chunk Size (~1000 chars):\n",
    "    This size keeps text chunks small enough to fit comfortably within typical LLM context windows (e.g., GPT-3.5-turbo has ~4,000 tokens max). 1000 characters roughly correspond to about 750 tokens, allowing space for prompt and response tokens during retrieval-augmented generation (RAG). It also preserves meaningful semantic units, usually fitting one or a few paragraphs.\n",
    "\n",
    "- Overlap (~100 chars):\n",
    "    Overlapping chunks by about 10% ensures that context spanning chunk boundaries is not lost. This helps the LLM capture continuous meaning when two chunks share important information at their edges (e.g., partial sentences or concepts). Overlap mitigates information fragmentation without creating excessive redundancy.\n",
    "\n",
    "- Recursive Splitting Logic:\n",
    "    The RecursiveCharacterTextSplitter tries to split on natural boundaries in hierarchical order: paragraph breaks (\"\\n\\n\"), line breaks (\"\\n\"), sentences (\".\"), spaces, and finally characters if needed. This helps maintain semantic coherence inside chunks rather than arbitrary cuts. If chunks are still too large after first boundary splitting, it moves to finer separators recursively until chunk size is met.\n",
    "\n",
    "- Handling Large Documents:\n",
    "    For very long books with many pages, this chunk size provides manageable retrieval units for vector search, optimizing between granularity and retrieval efficiency. Too large chunks slow retrieval and reduce semantic precision; too small chunks cause loss of context and increase index size.\n",
    "\n",
    "\n",
    "#### In summary\n",
    "\n",
    "Using chunk_size=1000 and chunk_overlap=100 with the recursive splitter targets natural semantic boundaries, fits well within typical context window limits, and ensures good context preservation across chunks in long PDF documents like books.\n",
    "\n",
    "This configuration is a practical balance for RAG systems ingesting large, complex documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a6f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split raw documents into 8929 chunks\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split documents into manageable chunks\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    chunks = splitter.split_documents(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Split raw documents into {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5aecbe",
   "metadata": {},
   "source": [
    "### Add Metadata for Traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5a648",
   "metadata": {},
   "source": [
    "#### Why Add Source Filename Metadata to Each Chunk?\n",
    "\n",
    "We add metadata to each chunk, such as the source filename.\n",
    "\n",
    "This metadata helps us track information provenance, supports debugging, and enables targeted queries by source.\n",
    "\n",
    "When large documents (like PDFs) are split into smaller pieces called chunks for processing and retrieval, it‚Äôs important to keep track of where each chunk originally came from.\n",
    "\n",
    "This metadata serves multiple important purposes:\n",
    "\n",
    "- Traceability: It helps you know which book, interview guide, or document a particular chunk belongs to. This is useful for debugging and validation.\n",
    "\n",
    "- Filtering: You can filter or prioritize search results by source if needed (for example, only search chunks from a specific book).\n",
    "\n",
    "- Context: It gives context to answers generated from these chunks ‚Äî you can tell users exactly which document the answer came from.\n",
    "\n",
    "- Organization: Helps maintain a well-organized index for the vector database, improving retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5333b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Generative  AI  for  Everyone  [Free  E-Book]   Natural  Language  Processing  3 Key  areas  of  NLP  include:  3 How  is  NLP  Useful  in  Generative  AI?  3 Examples  of  NLP  in  Generative  AI  Tools  4 AI  systems  and  tools  5 The  AI  layers  6 1.  Supervised  learning  8 2.  Unsupervised  learning  9 3.  Semi-supervised  learning  9 Deep  learning  10 Here's  how  ANNs  work:  10 Introduction  to  generative  AI  13 Generative  AI:  Some  fascinating  metrics  15 How  generative  AI  works  18 ML  model  vs.  gen  AI  model  19 Journey  from  traditional  programming  to  neural  networks  to  generative  AI  20 Large  Language  Models:  Powering  generative  AI  22 How  do  LLMs  work?  22 Different  Large  Language  Models  23 Components  of  an  LLM  25 How  do  LLMs  learn?  26 Building  an  LLM  application  27 LLMs  use  cases  28 Content  creation  28 Education  29 Customer  service  and  support  29 Research  and  development.  29 Entertainment  and  media  29\", metadata={'source': \"data/Generative AI_ A Beginner's Guide.pdf\", 'page': 0}),\n",
       " Document(page_content='Customer  service  and  support  29 Research  and  development.  29 Entertainment  and  media  29 Limitations  of  LLMs  30 LLM  Hallucinations  30 LLM  hallucination  mitigation  strategies  32 1.  Fine-tuning  32 2.  Prompt  engineering  33 3.  Retrieval  Augmented  Generation  (RAG)  33 RAG  chunking  strategies  35 Developing  a  Large  Language  Model  36', metadata={'source': \"data/Generative AI_ A Beginner's Guide.pdf\", 'page': 0}),\n",
       " Document(page_content='Vector  databases  37 Vector  Embeddings  38 How  vector  databases  work  39 Traditional  databases  vs.  vector  databases  39 The  vector  database  landscape  41 How  vector  databases  search  and  retrieve  data  42 Some  notable  generative  AI  frameworks  +  tools  44 LangChain  44 LlamaIndex  46 How  LlamaIndex  works  46 Hugging  Face  47 1.  Model  Hub:  47 2.  Datasets:  47 3.  Model  Training  &  Fine-tuning  Tools:  48 4.  Application  Building:  48 5.  Community  &  Collaboration:  48 The  Rise  of  Small  Language  Models  49 Size:  50 Focus:  50 Limitations:  50 Prompt  Engineering  51 Generative  AI  Developer  Stack  53 Using  Generative  AI  Responsibly  54 Best  Practices  for  Responsible  Generative  AI  Use:  54 Data:  54 Development  and  Deployment:  55 Content  and  User  Interaction:  55 Societal  Impact  and  Governance:  55 Multimodal  Models  56 Examples  of  multimodal  models  57 LLM-Powered  Autonomous  AI  Agents  57 Deploying  GenAI  Applications', metadata={'source': \"data/Generative AI_ A Beginner's Guide.pdf\", 'page': 1})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add source filename metadata to each chunk for traceability\n",
    "for chunk in all_chunks:\n",
    "    # The source is available in original doc metadata if present, else fallback\n",
    "    source = chunk.metadata.get(\"source\", \"unknown\")\n",
    "    chunk.metadata[\"source\"] = source\n",
    "\n",
    "# Confirm metadata assignment for first few chunks\n",
    "all_chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312cf6d",
   "metadata": {},
   "source": [
    "###  Summarize Dataset Statistics\n",
    "\n",
    "At this point, we can review key statistics such as total number of documents ingested, total chunks generated, and average chunk size.\n",
    "\n",
    "These insights inform us about dataset scale and potential enhancements in chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57487b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source PDF documents: 15\n",
      "Total text chunks created: 8929\n",
      "Average chunk length (characters): 744.92\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display basic dataset stats\n",
    "num_docs = len(pdf_files)\n",
    "num_chunks = len(all_chunks)\n",
    "avg_chunk_len = sum(len(chunk.page_content) for chunk in all_chunks)/num_chunks\n",
    "\n",
    "print(f\"Number of source PDF documents: {num_docs}\")\n",
    "print(f\"Total text chunks created: {num_chunks}\")\n",
    "print(f\"Average chunk length (characters): {avg_chunk_len:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fa078",
   "metadata": {},
   "source": [
    "### Next Steps and Integration\n",
    "\n",
    "After ingestion and chunking, these text chunks will be embedded and stored in our ChromaDB vector database for hybrid semantic retrieval.\n",
    "\n",
    "In subsequent notebooks, we will focus on embedding generation, vector store management, multi-agent orchestration, and building a user-facing query interface.\n",
    "\n",
    "This will help us build a fast, accurate RAG system powered by our curated document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5cab8",
   "metadata": {},
   "source": [
    "### Save Chunk Data to Disk After Chunking\n",
    "\n",
    "Serialize the list of chunk objects (e.g., as JSON, pickle, or a lightweight custom format).\n",
    "\n",
    "Save texts and metadata so they can be reloaded later for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d75924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved to research_notebooks/processed/chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"research_notebooks/processed\", exist_ok=True)\n",
    "\n",
    "with open(\"research_notebooks/processed/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(\"Chunks saved to research_notebooks/processed/chunks.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
