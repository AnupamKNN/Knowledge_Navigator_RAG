{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34eb4de5",
   "metadata": {},
   "source": [
    "## <b><center>Embedding & Vector Store Integration</center></b>\n",
    "Welcome to the second phase of the InfoFusion Technologies Multi-Agent RAG project!\n",
    "In this notebook, we will transform our curated text chunks into powerful numeric embeddings, store them in a high-performance vector database, and set the stage for fast, accurate semantic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb1536",
   "metadata": {},
   "source": [
    "### **Loading Cleaned Data Chunks**\n",
    "\n",
    "Before diving into embeddings, let's load the chunked dataset saved in the previous notebook.\n",
    "\n",
    "_This step ensures that our workflow is modular and data is reproducibleâ€”no need to rerun heavy ingestion or chunking!_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cef8273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8929 chunks from processed/chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "chunk_path = \"processed/chunks.pkl\"\n",
    "\n",
    "if os.path.exists(chunk_path):\n",
    "    with open(chunk_path, \"rb\") as f:\n",
    "        all_chunks = pickle.load(f)\n",
    "    print(f\"Loaded {len(all_chunks)} chunks from {chunk_path}\")\n",
    "else:\n",
    "    print(f\"Chunk file not found: {chunk_path}. Please run EDA notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec5710",
   "metadata": {},
   "source": [
    "### **Choosing our Embedding Engine**\n",
    "\n",
    "Embeddings are the backbone of any semantic search system.  \n",
    "\n",
    "We'll select a suitable model (e.g., Sentence Transformers, OpenAI, or Hugging Face) for generating vector representations of our text chunks.\n",
    "\n",
    "The right embedding choice can drastically impact retrieval quality and downstream agent performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860e690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anupam9k/AI_Resume_Projects/Book_Summarizer_and_Interview_Prep_Assistant/bienv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Select an embedding model ('all-MiniLM-L6-v2' is fast and effective for semantic search)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a497c",
   "metadata": {},
   "source": [
    "### **Vectorizing Knowledge: Embedding Generation**\n",
    "\n",
    "Now, let's transform each chunk into its corresponding numerical vector.  \n",
    "\n",
    "We'll process the text, ensure efficient batching, and monitor performance.\n",
    "\n",
    "The batch size of 32 in embedder.encode(texts, batch_size=32, show_progress_bar=True) is chosen because it provides an optimal balance between throughput (speed) and memory usage for most hardware and models, including the widely used all-MiniLM-L6-v2 Sentence Transformer.\n",
    "\n",
    "Batching enables processing multiple text chunks simultaneously, leveraging parallelism on GPUs or CPUs for efficient embedding generation. While higher batch sizes (e.g., 64, 128) can speed up embedding, they also consume more memory and risk instability or out-of-memory errors on limited hardware. For common configurations, batch sizes of 16 or 32 are often recommended as the \"sweet spot\" for good speed without exceeding memory limits.\n",
    "\n",
    "\n",
    "This step empowers the system to capture semantic meaning, enabling intelligent and context-aware retrieval later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecdb26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 8929 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280/280 [00:10<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8929 embeddings.\n"
     ]
    }
   ],
   "source": [
    "texts = [chunk.page_content for chunk in all_chunks]\n",
    "print(f\"Computing embeddings for {len(texts)} chunks.\")\n",
    "\n",
    "# Efficient batch embedding\n",
    "embeddings = embedder.encode(texts, batch_size = 32, show_progress_bar = True)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb4a36",
   "metadata": {},
   "source": [
    "### **Building the Vector Store with ChromaDB**\n",
    "\n",
    "With embeddings ready, we'll store them in ChromaDBâ€”a fast, scalable vector database built for RAG workloads.\n",
    "\n",
    "Storing embeddings in ChromaDB allows instant, similarity-based retrieval, powering multi-agent workflows and conversational AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac13dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'infofusion_chunks' created.\n",
      "Added 8929 chunks to ChromaDB collection.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize PersistentClient for local persistent storage\n",
    "client = chromadb.PersistentClient(\n",
    "    path=\"db/chromadb_data\",\n",
    "    settings=Settings()\n",
    ")\n",
    "\n",
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(name=\"infofusion_chunks\")\n",
    "print(f\"Collection '{collection.name}' created.\")\n",
    "\n",
    "# Prepare documents with IDs and metadata\n",
    "docs_to_add = []\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    doc = {\n",
    "        \"id\": str(i),\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"document\": chunk.page_content,\n",
    "        \"metadata\": chunk.metadata\n",
    "    }\n",
    "    docs_to_add.append(doc)\n",
    "\n",
    "\n",
    "# Add documents in batch (recommended) instead of one-by-one\n",
    "collection.add(\n",
    "    ids=[doc[\"id\"] for doc in docs_to_add],\n",
    "    embeddings=embeddings,\n",
    "    documents=[doc[\"document\"] for doc in docs_to_add],\n",
    "    metadatas=[doc[\"metadata\"] for doc in docs_to_add]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Added {len(docs_to_add)} chunks to ChromaDB collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414be9a",
   "metadata": {},
   "source": [
    "### **Smart Searches in Action**\n",
    "\n",
    "Letâ€™s run a few sample queries to validate that our semantic search pipeline is functioning as expected.\n",
    "\n",
    "We'll retrieve top-matching chunks for sample questions or keywords, demonstrating the power of our new vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d36dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 retrieved chunks for query:\n",
      "---\n",
      "data/Artificial Intelligence, Machine Learning, and Deep Learning.pdf\n",
      "106 â€¢ Artifici Al intelligence , MAchine  leArning , Deep leArning\n",
      "ful for understanding ANNs. A better way to understand ANNs is to think  \n",
      "of their structure as a combination of the hyper parameters in the  \n",
      "following list:\n",
      "â€¢\t The number of hidden layers\n",
      "â€¢\t The number of neurons in each hidden layer\n",
      "â€¢\t The initial weights of edges connecting pairs of neurons\n",
      "â€¢\t The activation function\n",
      "â€¢\t A cost (a.k.a. loss) function\n",
      "â€¢\t An optimizer (used with the cost function)\n",
      "â€¢\t The learning rate (a small number)\n",
      "â€¢\t The dropout rate (optional)\n",
      "Figure 4.2 displays the contents of an ANN (there are many variations: \n",
      "this is simply one example).\n",
      "FIGURE 4.2 An Example of an ANN.\n",
      "Image adapted from [Cburnett, Source: https://commons.wikimedia.\n",
      "org/wiki/File:Artificial_neural_network.svg] ...\n"
     ]
    }
   ],
   "source": [
    "# Query example: \"Explain gradient descent\"\n",
    "query = \"Explain ann.\"\n",
    "query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "\n",
    "# Search top 3 matches in ChromaDB\n",
    "results = collection.query(\n",
    "    query_embeddings= [query_embedding],\n",
    "    n_results=1\n",
    ")\n",
    "\n",
    "print(\"Top 3 retrieved chunks for query:\")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(\"---\")\n",
    "    print(meta[\"source\"])\n",
    "    print(doc, '...') # Shows first 500 characters of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bf07e",
   "metadata": {},
   "source": [
    "### **Pipeline Stats & Housekeeping**\n",
    "\n",
    "Finally, letâ€™s log important pipeline metadataâ€”such as embedding times, chunk counts, and indexing statsâ€”and clean up any intermediate artifacts.\n",
    "\n",
    "Tracking these metrics ensures transparency, reproducibility, and provides insights for future optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511f27ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 8929\n",
      "Embedding dimensions: 384\n",
      "ChromaDB collection count : 8929\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of chunks: {len(all_chunks)}\")\n",
    "print(f\"Embedding dimensions: {embeddings[0].shape[0] if len(embeddings) > 0 else 'N/A'}\")\n",
    "print(f\"ChromaDB collection count : {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2ee25",
   "metadata": {},
   "source": [
    "#### ðŸ“Š Collection Consistency Check\n",
    "\n",
    "From the summary statistics above, we can confirm that our pipeline has successfully created and indexed the data as intended:\n",
    "\n",
    "- **Total number of text chunks:** 8929  \n",
    "- **Embedding vector dimensions:** 384  \n",
    "- **Number of records in ChromaDB collection:** 8929\n",
    "\n",
    "This perfect alignment between the number of generated text chunks and the indexed entries in ChromaDB demonstrates robust data integrity.  \n",
    "It ensures that every processed document chunk is now ready for high-performance semantic retrieval in our RAG system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
