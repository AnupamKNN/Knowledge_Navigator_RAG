{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c5ecc0",
   "metadata": {},
   "source": [
    "## <b><center>Multi-Agent Retrieval-Augmented Generation Orchestration</center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe964f61",
   "metadata": {},
   "source": [
    "Welcome to the third phase of the InfoFusion Technologies Multi-Agent RAG project!  \n",
    "\n",
    "In this notebook, we will build the orchestration layer that brings together our embedded knowledge base and internet search capabilities.  \n",
    "Using multiple specialized agents, we will enable comprehensive retrieval, question answering, and intelligent response synthesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadef9a",
   "metadata": {},
   "source": [
    "### üß†**Load Vector Store and Embedding Model**\n",
    "\n",
    "Before building retrieval agents, we first reload the ChromaDB collection containing our embedded knowledge base, and initialize the embedding model for semantic queries.\n",
    "\n",
    "This step ensures our agents operate on the latest indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeef08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anupam9k/AI_Resume_Projects/Book_Summarizer_and_Interview_Prep_Assistant/bienv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, SerpAPIWrapper\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b4c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n",
      "Loaded ChromaDB collection with 8929 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# Connect to ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"db/chromadb_data\")\n",
    "collection = client.get_collection(name=\"infofusion_chunks\")\n",
    "print(f\"Loaded ChromaDB collection with {collection.count()} embeddings.\")\n",
    "\n",
    "# Initialize Wikipedia and SerpAPI agents\n",
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
    "wikipedia_agent = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "\n",
    "serp_api_key = os.environ.get(\"SERP_API_KEY\")\n",
    "serp_api_wrapper = SerpAPIWrapper(serpapi_api_key=serp_api_key)\n",
    "\n",
    "# Agent list prioritized: Wikipedia first, then SerpAPI\n",
    "# internet_search_agents = [wikipedia_agent, serp_api_wrapper]\n",
    "\n",
    "internet_search_agents = {\n",
    "    \"Wikipedia\": wikipedia_agent,\n",
    "    \"SerpAPI\": serp_api_wrapper\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67e643",
   "metadata": {},
   "source": [
    "### üîç **Define Semantic Retrieval Agent**\n",
    "\n",
    "This agent performs similarity search over our embedded documents using ChromaDB.\n",
    "\n",
    "Given a natural language query, it retrieves relevant text chunks that will form the knowledge base context for downstream reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafeaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 docs retrieved:\n",
      "110 ‚Ä¢ Artifici Al intelligence , MAchine  leArning , Deep leArning\n",
      "Here is a key point: backward error propagation involves the cal -\n",
      "culation of numbers that are used to update the weights of the edges in \n",
      "the neural network . The update process is performed by means of a loss \n",
      "function (and an optimizer and a learning rate), starting from the output \n",
      "layer (the right-most layer) and then moving in a right-to-left  fashion \n",
      "in order to update the weights of the edges between consecutive lay -\n",
      "ers. This procedure trains the neural network, which involves reducing \n",
      "the loss between the estimated values at the output layer and the true \n",
      "values (in the case of supervised learning). This procedure is repeated \n",
      "for each data point in the training portion of the dataset. Processing the \n",
      "dataset is called an epoch, and many times a neural network is trained \n",
      "via multiple epochs.\n",
      "The previous paragraph did not explain what the loss function is or ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_retrieval(query, top_k = 1):\n",
    "    query_emb = embedder.encode([query])[0] \n",
    "    results = collection.query(\n",
    "        query_embeddings = [query_emb],\n",
    "        n_results = top_k\n",
    "    )\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Test retrieval agent\n",
    "sample_query = \"What is gradient descent?\"\n",
    "retrieved_docs = semantic_retrieval(sample_query)\n",
    "print(f\"Top {len(retrieved_docs)} docs retrieved:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc, \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc81f7a",
   "metadata": {},
   "source": [
    "### üåê Initialize Internet Search Agents\n",
    "\n",
    "To complement static documents, we integrate real-time external knowledge sources like Wikipedia and general web search through SerpAPI.  \n",
    "These agents ensure we have the latest, broad knowledge at our fingertips beyond our vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1936958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Wikipedia response:\n",
      "Page: Gradient\n",
      "Summary: In vector calculus, the gradient of a scalar-valued differentiable function \n",
      "  \n",
      "    \n",
      "      \n",
      "        f\n",
      "      \n",
      "    \n",
      "    {\\displaystyle f}\n",
      "  \n",
      " of several variables is the vector field (or vector-valued function) \n",
      "  \n",
      "    \n",
      "      \n",
      "        ‚àá\n",
      "        f\n",
      "      \n",
      "    \n",
      "    {\\displaystyle \\nabla f}\n",
      "  \n",
      " whose value at a point \n",
      "  \n",
      "    \n",
      "      \n",
      "        p\n",
      "      \n",
      "    \n",
      "    {\\displaystyle p}\n",
      "  \n",
      " gives the direction and the rate of fastest increase. The gradient transforms like a vector under c\n",
      "\n",
      "Agent SerpAPI response:\n",
      "['Gradient descent is an optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results.', 'Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate ...', 'Gradient Descent is a fundamental algorithm in machine learning and optimization. It is used for tasks like training neural networks, fitting regression lines, ...', 'Gradient descent is a mathematical technique that iteratively finds the weights and bias that produce the model with the lowest loss. Gradient descent finds ...', 'Gradient descent is an optimization algorithm used in machine learning to minimize the cost function by iteratively adjusting parameters in the direction of ...', 'Gradient descent is a general-purpose algorithm that numerically finds minima of multivariable functions.', \"Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters ...\", 'Gradient descent is the backbone of the learning process for various algorithms, including linear regression, logistic regression, support vector machines, and ...', 'Gradient descent is an optimization algorithm that minimizes a cost function, powering models like linear regression and neural networks.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def internet_search(query):\n",
    "    \"\"\"\n",
    "    Performs an internet search using multiple agents and aggregates the results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for agent_name, agent_object in internet_search_agents.items():\n",
    "        try:\n",
    "            # We call the 'run' method on the agent object, not the agent name\n",
    "            response = agent_object.run(query)\n",
    "            results[agent_name] = response\n",
    "        except Exception as e:\n",
    "            print(f\"Agent {agent_name} failed with error: {e}\")\n",
    "    return results\n",
    "\n",
    "# Test internet search function\n",
    "sample_query = \"What is gradient descent?\"\n",
    "internet_search_results = internet_search(sample_query)\n",
    "\n",
    "# Now, iterate over the returned dictionary to print the results\n",
    "for agent_name, result in internet_search_results.items():\n",
    "    print(f\"Agent {agent_name} response:\\n{result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b8f34",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Wrap Retrieval and Internet Agents as Tools\n",
    "\n",
    "We transform each knowledge source into a LangChain Tool with clear names and descriptions.  \n",
    "This modular tool design empowers the language model to flexibly call on the best resource for each query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7667d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "class VectorDBTool(BaseTool):\n",
    "    name = \"VectorDB\"\n",
    "    description = \"Searches for answers in the embedded document database.\"\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        chunks = semantic_retrieval(query, top_k=3)\n",
    "        return \"\\n---\\n\".join(chunks)\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "class WikipediaTool(BaseTool):\n",
    "    name = \"Wikipedia\"\n",
    "    description = \"Performs Wikipedia search to answer questions.\"\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            return wikipedia_agent.run(query)\n",
    "        except Exception as e:\n",
    "            return f\"Wikipedia Agent error: {e}\"\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "class SerpApiTool(BaseTool):\n",
    "    name = \"SerpAPI\"\n",
    "    description = \"Uses SerpAPI for live web search.\"\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            return serp_api_wrapper.run(query)\n",
    "        except Exception as e:\n",
    "            return f\"SerpAPI Agent error: {e}\"\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "tools = [VectorDBTool(), WikipediaTool(), SerpApiTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced2000",
   "metadata": {},
   "source": [
    "### ü§ñ Prioritized Orchestrator Function\n",
    "\n",
    "Our orchestrator queries each tool in priority order:  \n",
    "- VectorDB first, checking for substantial content,  \n",
    "- then Wikipedia,  \n",
    "- finally SerpAPI for live web results.  \n",
    "\n",
    "This approach balances depth, recency, and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b57b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritized_orchestrator(query):\n",
    "    # Query VectorDB first\n",
    "    vec_res = VectorDBTool()._run(query)\n",
    "    # Confidence: require minimum length; can be extended to semantic quality or keywords\n",
    "    if vec_res and len(vec_res.strip()) > 50:\n",
    "        return vec_res\n",
    "\n",
    "    # Fallback to Wikipedia search\n",
    "    wiki_res = WikipediaTool()._run(query)\n",
    "    if wiki_res and any(char.isalnum() for char in wiki_res):\n",
    "        return wiki_res\n",
    "\n",
    "    # Final fallback to SerpAPI web search\n",
    "    serp_res = SerpApiTool()._run(query)\n",
    "    if serp_res and any(char.isalnum() for char in serp_res):\n",
    "        return serp_res\n",
    "\n",
    "    return \"Sorry, no good answer found from any source.\"\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# user_query = \"What is gradient descent?\"\n",
    "# answer = prioritized_orchestrator(user_query)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa28c6",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Design a Custom Prompt Template\n",
    "\n",
    "We craft a carefully worded prompt that instructs ChatGroq how to prioritize tools for answering:  \n",
    "\n",
    "1) Check VectorDB first, \n",
    "2) Fallback to Wikipedia, \n",
    "3) Finally try SerpAPI if needed.  \n",
    "\n",
    "This prompt guides the model to provide accurate and concise answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88b71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "custom_prompt = \"\"\"\n",
    "You are an expert AI assistant with access to three powerful tools:\n",
    "\n",
    "- **VectorDB:** An internal vector database containing high-quality, trusted documents and knowledge, preferred for all answers when possible.\n",
    "- **Wikipedia:** A tool to search Wikipedia for up-to-date factual content when the internal database does not fully answer the question.\n",
    "- **SerpAPI:** A live internet search tool for real-time and broad web knowledge, used only if neither VectorDB nor Wikipedia provide sufficient information.\n",
    "\n",
    "**For every user request:**\n",
    "1. Carefully consider the user's question, reduce it to its core intent if needed.\n",
    "2. Query VectorDB for the most semantically relevant content chunks.\n",
    "      - If the retrieved content directly and confidently answers the user's question, use ONLY this content. Compose a precise, well-written answer using and summarizing these chunks. Cite/refer to the found content in your synthesis.\n",
    "3. If VectorDB does not contain enough or sufficiently relevant information, use the Wikipedia tool.\n",
    "      - Integrate the best Wikipedia result(s) clearly and accurately with any helpful context you have from earlier steps.\n",
    "4. If both VectorDB and Wikipedia fail to contain the required answer, use SerpAPI as a last resort.\n",
    "      - When using SerpAPI, prioritize concise, factual, and up-to-date responses.\n",
    "5. In all cases, create a direct, complete answer that directly addresses the user's actual question.\n",
    "      - Explain reasoning step by step if helpful for clarity, but avoid any filler, speculation, or unsupported content.\n",
    "      - Ground the answer in the retrieved sources and cite their origin (e.g., \"According to VectorDB\", \"Wikipedia states...\", etc.).\n",
    "      - Do NOT hallucinate or invent information beyond what is retrieved.\n",
    "6. If no sources provide a sufficient answer, honestly state that the information could not be found.\n",
    "\n",
    "**Remember:**  \n",
    "- Always try VectorDB first and prefer it for trusted, detailed content.\n",
    "- Only use external search (Wikipedia, SerpAPI) if necessary, and integrate results transparently and carefully.\n",
    "- Answer in a more natural manner, and do not mention things like \"According to VectorDB\", \"Wikipedia states...\", etc.\n",
    "\n",
    "**User Question:**  \n",
    "{input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt, input_variables=[\"input\", \"agent_scratchpad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9d6b7",
   "metadata": {},
   "source": [
    "### ü§ñ Instantiate ChatGroq Agent Executor\n",
    "\n",
    "Now we start up our ChatGroq LLM with the tools and prompt, assembling the complete multi-agent orchestrator.  \n",
    "\n",
    "This agent intelligently decides which knowledge source to query for the best answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de6205c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key = groq_api_key, model = \"llama-3.3-70b-versatile\")\n",
    "\n",
    "agent = create_openai_tools_agent(llm = llm, tools = tools, prompt = prompt)\n",
    "\n",
    "agent_executer = AgentExecutor(agent= agent, tools = tools, verbose= True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658cc0b",
   "metadata": {},
   "source": [
    "### ‚úÖ Test Direct Q&A with Multi-Agent RAG\n",
    "\n",
    "Let‚Äôs put it all together by asking a question and seeing how our system dynamically queries multiple knowledge bases.  \n",
    "\n",
    "The answer synthesis showcases the power of combining internal and external intelligence seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03498d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `VectorDB` with `PCA`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mintroDuction  to MAchine  leArning  ‚Ä¢ 33\n",
      "There are two advantages to PCA: 1) reduced computation time due to \n",
      "far fewer features and 2) the ability to graph the components when there are \n",
      "at most three components. If you have four or five components, you won‚Äôt be \n",
      "able to display them visually, but you could select subsets of three components \n",
      "for visualization, and perhaps gain some additional insight into the dataset.\n",
      "PCA uses the variance as a measure of information: the higher the variance, \n",
      "the more important the component. In fact, just to jump ahead slightly: PCA \n",
      "determines the eigenvalues and eigenvectors of a covariance matrix (discussed \n",
      "later), and constructs a new matrix whose columns are eigenvectors, ordered \n",
      "from left-to-right based on the maximum eigenvalue in the left-most column, \n",
      "decreasing until the right-most eigenvector also has the smallest eigenvalue.\n",
      "Covariance Matrix\n",
      "As a reminder, the statistical quantity called the variance of a random vari-\n",
      "---\n",
      "which includes t-Distributed Stochastic Neighbor Embedding (t-SNE) as \n",
      "well as UMAP .\n",
      "This chapter discusses PCA, and you can perform an online search to \n",
      "find more information about the other algorithms.\n",
      "PCA\n",
      "Principal Components are new components that are linear combinations of \n",
      "the initial variables in a dataset. In addition, these components are uncor -\n",
      "related and the most meaningful or important information is contained in \n",
      "these new components.\n",
      "---\n",
      "Parse tree, 16, 17\n",
      "PCA. See Principal component analysis\n",
      "PeepholeLSTMCell class, 191\n",
      "Perceptron activation function, 104\n",
      "Perceptron Learning Rule, 13\n",
      "Perceptrons, 103‚Äì105\n",
      "in artificial neural network, 105\n",
      "detailed view of, 104\n",
      "function, 104\n",
      "Persistent Gradient Tape, 257‚Äì258\n",
      "‚ÄúPerturbation technique,‚Äù 46, 48‚Äì50, 54, 55, 62\u001b[0m\u001b[32;1m\u001b[1;3mPrincipal Component Analysis (PCA) is a technique used in machine learning to reduce the dimensionality of a dataset by transforming the original features into new, uncorrelated components called principal components. These components are linear combinations of the initial variables and contain the most meaningful or important information in the dataset. The goal of PCA is to identify the principal components that describe the variance within the data, with the first principal component explaining the most variance, and subsequent components explaining less. This is achieved by determining the eigenvalues and eigenvectors of a covariance matrix and constructing a new matrix whose columns are eigenvectors, ordered from left to right based on the maximum eigenvalue. The advantages of PCA include reduced computation time due to fewer features and the ability to graph the components when there are at most three components, allowing for visualization and potential insight into the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent response:\n",
      " {'input': 'What is PCA?', 'output': 'Principal Component Analysis (PCA) is a technique used in machine learning to reduce the dimensionality of a dataset by transforming the original features into new, uncorrelated components called principal components. These components are linear combinations of the initial variables and contain the most meaningful or important information in the dataset. The goal of PCA is to identify the principal components that describe the variance within the data, with the first principal component explaining the most variance, and subsequent components explaining less. This is achieved by determining the eigenvalues and eigenvectors of a covariance matrix and constructing a new matrix whose columns are eigenvectors, ordered from left to right based on the maximum eigenvalue. The advantages of PCA include reduced computation time due to fewer features and the ability to graph the components when there are at most three components, allowing for visualization and potential insight into the dataset.', 'intermediate_steps': [(ToolAgentAction(tool='VectorDB', tool_input='PCA', log='\\nInvoking: `VectorDB` with `PCA`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'id': 'na7hjtx7q', 'function': {'arguments': '{\"__arg1\":\"PCA\"}', 'name': 'VectorDB'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f454ed04-b26c-423d-9f10-62be21a20a0f', tool_calls=[{'name': 'VectorDB', 'args': {'__arg1': 'PCA'}, 'id': 'na7hjtx7q'}], tool_call_chunks=[{'name': 'VectorDB', 'args': '{\"__arg1\":\"PCA\"}', 'id': 'na7hjtx7q', 'index': None}])], tool_call_id='na7hjtx7q'), 'introDuction  to MAchine  leArning  ‚Ä¢ 33\\nThere are two advantages to PCA: 1) reduced computation time due to \\nfar fewer features and 2) the ability to graph the components when there are \\nat most three components. If you have four or five components, you won‚Äôt be \\nable to display them visually, but you could select subsets of three components \\nfor visualization, and perhaps gain some additional insight into the dataset.\\nPCA uses the variance as a measure of information: the higher the variance, \\nthe more important the component. In fact, just to jump ahead slightly: PCA \\ndetermines the eigenvalues and eigenvectors of a covariance matrix (discussed \\nlater), and constructs a new matrix whose columns are eigenvectors, ordered \\nfrom left-to-right based on the maximum eigenvalue in the left-most column, \\ndecreasing until the right-most eigenvector also has the smallest eigenvalue.\\nCovariance Matrix\\nAs a reminder, the statistical quantity called the variance of a random vari-\\n---\\nwhich includes t-Distributed Stochastic Neighbor Embedding (t-SNE) as \\nwell as UMAP .\\nThis chapter discusses PCA, and you can perform an online search to \\nfind more information about the other algorithms.\\nPCA\\nPrincipal Components are new components that are linear combinations of \\nthe initial variables in a dataset. In addition, these components are uncor -\\nrelated and the most meaningful or important information is contained in \\nthese new components.\\n---\\nParse tree, 16, 17\\nPCA. See Principal component analysis\\nPeepholeLSTMCell class, 191\\nPerceptron activation function, 104\\nPerceptron Learning Rule, 13\\nPerceptrons, 103‚Äì105\\nin artificial neural network, 105\\ndetailed view of, 104\\nfunction, 104\\nPersistent Gradient Tape, 257‚Äì258\\n‚ÄúPerturbation technique,‚Äù 46, 48‚Äì50, 54, 55, 62')]}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is PCA?\"\n",
    "\n",
    "response = agent_executer.invoke({\"input\": user_question})\n",
    "\n",
    "print(\"Agent response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa17611",
   "metadata": {},
   "source": [
    "### üöÄ Next Steps\n",
    "\n",
    "- **Modularize the codebase** to enhance maintainability and scalability.  \n",
    "- **Integrate the logic into an interactive Streamlit application** featuring a multi-tab user interface for an enhanced user experience.  \n",
    "- **Develop MCQ creation and automated answer evaluation features** by leveraging similar orchestrator design patterns.  \n",
    "- **Experiment with advanced confidence metrics and agent-based LLM conversation chains** to improve accuracy and interactivity.  \n",
    "- **Deploy your Retrieval-Augmented Generation (RAG) assistant** either as a cloud-hosted API or a fully containerized application for seamless accessibility and scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
